import torch
torch.backends.cuda.preferred_linalg_library("default")
import os
import json
from torch import nn
from timeit import default_timer as timer
from tqdm.auto import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score
import argparse
from datetime import datetime as dt
import torch.optim as optim
import itertools
import pandas as pd
import traceback
import sys
sys.path.append("..")
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from data_loader_java import *
from data_loader_c import *
from data_loader_cpp import *
from torch.utils.tensorboard import SummaryWriter



device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")
torch.manual_seed(42)
torch.cuda.manual_seed(42)
model_name = "SimpleNN"


class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, dropout_rate):
        super().__init__()
        
        if not isinstance(hidden_size, list):
            hidden_size = [hidden_size]
        
        layers = []
        layers.append(nn.Sigmoid())
        layers.append(nn.Linear(input_size, hidden_size[0]))
        layers.append(nn.BatchNorm1d(hidden_size[0]))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(dropout_rate))

        for i in range(len(hidden_size)-1):
            layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))
            layers.append(nn.BatchNorm1d(hidden_size[i+1]))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout_rate))

        layers.append(nn.Linear(hidden_size[-1], 1))

        self.model = nn.Sequential(*layers)
    
        layers = []



    def forward(self, x):
        return self.model(x).squeeze()
    



def save_hyperparameters(hyperparams, save_dir, model):

    serializable_params = {}
    for key, value in hyperparams.items():
        if isinstance(value, torch.nn.Module):
            serializable_params[key] = value.__class__.__name__
        elif isinstance(value, list):
            serializable_params[key] = value
        elif isinstance(value, type) and issubclass(value, torch.optim.Optimizer):
             serializable_params[key] = value.__name__ 
        elif isinstance(value, type) and issubclass(value, torch.nn.modules.loss._Loss):
             serializable_params[key] = value.__name__ 
        elif isinstance(value, (tuple, dict, str, int, float, bool, type(None))): 
             serializable_params[key] = value
        else: 
             serializable_params[key] = str(value)
    
    if model is not None:
        serializable_params['MODEL_ARCHITECTURE'] = str(model)
    
    os.makedirs(save_dir, exist_ok=True)
    
    hyperparam_path = os.path.join(save_dir, "hyperparameters.json")
    with open(hyperparam_path, 'w') as f:
        json.dump(serializable_params, f, indent=4)
    
    return hyperparam_path

def save_model_state(model, optimizer, scheduler, epoch, save_path, hyperparameters):
    checkpoint_path = os.path.join(save_path, f"model_checkpoint_epoch_{epoch}.pt")
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None
    }
    
    torch.save(checkpoint, checkpoint_path)
    
    if epoch == hyperparameters['EPOCHS'] - 1:
        final_model_path = os.path.join(save_path, "final_model.pt")
        torch.save(model.state_dict(), final_model_path)
    
    return checkpoint_path



class EarlyStopping:
    def __init__(self, patience, min_delta):

        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
        self.val_loss_min = float('inf')
        
    def __call__(self, val_loss, model, optimizer, scheduler, epoch, save_path):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model, optimizer, scheduler, epoch, save_path)
        elif val_loss > self.best_loss + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.save_checkpoint(val_loss, model, optimizer, scheduler, epoch, save_path)
            self.counter = 0
            
    def save_checkpoint(self, val_loss, model, optimizer, scheduler, epoch, save_path):
        save_model_state(model, optimizer, scheduler, epoch, save_path)
        self.val_loss_min = val_loss

def calculate_regression_metrics(targets, predictions):

    if isinstance(targets, torch.Tensor):
        targets = targets.cpu().detach().numpy()
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.cpu().detach().numpy()
    
    mse = mean_squared_error(targets, predictions)
    mae = mean_absolute_error(targets, predictions)
    r2 = r2_score(targets, predictions)
    
    return {
        'mse': mse,
        'mae': mae,
        'r_squared': r2
    }

def calculate_classification_metrics(targets, predictions, threshold):

    if isinstance(targets, torch.Tensor):
        targets = targets.cpu().detach().numpy()
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.cpu().detach().numpy()
    
    binary_predictions = (predictions >= threshold).astype(int)
    binary_targets = (targets >= threshold).astype(int)
    
    accuracy = accuracy_score(binary_targets, binary_predictions)
    precision = precision_score(binary_targets, binary_predictions, zero_division=0)
    recall = recall_score(binary_targets, binary_predictions, zero_division=0)
    f1 = f1_score(binary_targets, binary_predictions, zero_division=0)
    
    return {
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1
    }

def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               scheduler: torch.optim.lr_scheduler._LRScheduler,
               device: torch.device,
               is_regression: bool = False):

    model.train()
    
    train_loss = 0.0
    all_predictions = []
    all_targets = []
    
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device) 
        
        predictions = model(X)
        loss = loss_fn(predictions, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        train_loss += loss.item()
        
        all_predictions.append(predictions.cpu().detach())
        all_targets.append(y.cpu())
    
    scheduler.step()
    
    train_loss /= len(dataloader)
    
    all_predictions = torch.cat(all_predictions, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()

    if is_regression:
         metrics = calculate_regression_metrics(all_targets, all_predictions)
         metrics['loss'] = train_loss 
    else:
        metrics = calculate_classification_metrics(all_targets, all_predictions)
        metrics['loss'] = train_loss 
    
    return metrics

def test_step(model: torch.nn.Module, 
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              device: torch.device,
              is_regression: bool = False):

    model.eval()
    
    test_loss = 0.0
    all_predictions = []
    all_targets = []
    
    with torch.no_grad():
        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device).float()
            
            predictions = model(X)
            loss = loss_fn(predictions, y)
            
            test_loss += loss.item()
            
            all_predictions.append(predictions.cpu().detach())
            all_targets.append(y.cpu())
    
    test_loss /= len(dataloader)
    
    all_predictions = torch.cat(all_predictions, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()
    
    if is_regression:
         metrics = calculate_regression_metrics(all_targets, all_predictions)
         metrics['loss'] = test_loss 
    else:
        metrics = calculate_classification_metrics(all_targets, all_predictions)
        metrics['loss'] = test_loss 
    
    return metrics

class EnhancedCustomWriter:
    @staticmethod
    def create_Writer(hyperparameters, experiment_name, model_name, dataset_name, extra):

        timestamp = dt.now().strftime("%Y%m%d_%H%M")
        
        if dataset_name is None:
            language = hyperparameters.get('LANGUAGE', 'Java')
            scanner_type = hyperparameters.get('SCANNER_TYPE', 'semgrep')
            label_type = hyperparameters.get('LABEL_TYPE', 'vuln_score')
            hidden_sizes = "_".join(map(str, hyperparameters['HIDDEN_SIZES']))
            lr = hyperparameters.get('LEARNING_RATE', 0.001)
            dropout = hyperparameters.get('DROPOUT', 0.2)
            batch_size = hyperparameters.get('BATCH_SIZE', 32)
            optimizer_name = hyperparameters.get('OPTIMIZER', 'Adam').__name__ if isinstance(hyperparameters.get('OPTIMIZER'), type) else hyperparameters.get('OPTIMIZER', 'Adam')
            gamma = hyperparameters.get('SCHEDULER_GAMMA', 0.1)
            betas = hyperparameters.get('BETAS', (0.9, 0.999))
            betas_str = f"{betas[0]}_{betas[1]}"
            momentum = hyperparameters.get('MOMENTUM', 0.9)
            loss_fn = hyperparameters.get('LOSS_FUNCTION', 'BCELoss').__class__.__name__ if hasattr(hyperparameters.get('LOSS_FUNCTION', 'BCELoss'), '__class__') else hyperparameters.get('LOSS_FUNCTION', 'BCELoss')
            dataset_name = f"{language}_{scanner_type}_{label_type}_h{hidden_sizes}_lr{lr}_d{dropout}_b{batch_size}_opt{optimizer_name}_g{gamma}_b{betas_str}_m{momentum}_loss{loss_fn}"
        
        if extra:
            log_dir = f"runs/{experiment_name}/{model_name}/{dataset_name}/{timestamp}_{extra}"
        else:
            log_dir = f"runs/{experiment_name}/{model_name}/{dataset_name}/{timestamp}"
        
        hidden_sizes_str = "_".join(map(str, hyperparameters['HIDDEN_SIZES']))
        
        hyperparams_text = "# Model Hyperparameters\n\n"
        hyperparams_text += f"## Architecture\n"
        hyperparams_text += f"* **Model Type**: {model_name}\n"
        hyperparams_text += f"* **Input Size**: {hyperparameters['INPUT_SIZE']}\n"
        hyperparams_text += f"* **Hidden Sizes**: {hyperparameters['HIDDEN_SIZES']}\n"
        hyperparams_text += f"* **Hidden Layer Count**: {len(hyperparameters['HIDDEN_SIZES'])}\n"
        hyperparams_text += f"* **Dropout Rate**: {hyperparameters['DROPOUT']}\n\n"
        
        hyperparams_text += f"## Training Configuration\n"
        hyperparams_text += f"* **Learning Rate**: {hyperparameters['LEARNING_RATE']}\n"
        hyperparams_text += f"* **Batch Size**: {hyperparameters['BATCH_SIZE']}\n"
        hyperparams_text += f"* **Epochs**: {hyperparameters['EPOCHS']}\n"
        hyperparams_text += f"* **Optimizer**: {optim.Adam.__name__}\n"
        hyperparams_text += f"* **Loss Function**: {hyperparameters['LOSS_FUNCTION'].__class__.__name__}\n"
        
        scheduler_type = hyperparameters['SCHEDULER_STEP_SIZE']
        hyperparams_text += f"* **Learning Rate Scheduler**: {scheduler_type}\n"
        for param_name, param_value in hyperparameters.items():
            if param_name.startswith('SCHEDULER_'):
                hyperparams_text += f"  * **{param_name[11:]}**: {param_value}\n"
        
        hyperparams_text += f"\n## Dataset\n"
        hyperparams_text += f"* **Dataset**: {dataset_name}\n"
        hyperparams_text += f"* **Workers**: {hyperparameters['NUM_WORKERS']}\n"
        
        writer = SummaryWriter(
            log_dir=log_dir,
            comment='',
            purge_step=None,
            max_queue=1, 
            flush_secs=10 
        )
        
        writer.add_text("hyperparameters", hyperparams_text)
        
        clean_log_dir = log_dir.replace('runs/', '')
        
        summary_text = f"# {model_name} on {dataset_name}\n\n"
        summary_text += f"* **Date**: {timestamp}\n"
        summary_text += f"* **Hidden Layers**: {hidden_sizes_str}\n"
        summary_text += f"* **Epochs**: {hyperparameters['EPOCHS']}\n"
        summary_text += f"* **Learning Rate**: {hyperparameters['LEARNING_RATE']}\n"
        summary_text += f"* **Log Directory**: `{clean_log_dir}`\n"
        
        writer.add_text("model_card", summary_text)
        
        return writer, log_dir
    
    @staticmethod
    def get_logdir(writer):
        return writer.log_dir

def train(model: torch.nn.Module, 
          train_dataloader: torch.utils.data.DataLoader,
          test_dataloader: torch.utils.data.DataLoader,
          loss_fn: torch.nn.Module,
          optimizer: torch.optim.Optimizer,
          scheduler: torch.optim.lr_scheduler._LRScheduler,
          device: torch.device,
          epochs: int,
          writer: torch.utils.tensorboard.SummaryWriter,
          log_dir: str,
          save_interval: int = 10,
          patience: int = 7,
          hyperparameters: dict = None,
          is_regression: bool = False):
    
    if is_regression:
        history = {
            'train_loss': [], 'train_mae': [], 'train_mse': [], 'train_r_squared': [],
            'test_loss': [], 'test_mae': [], 'test_mse': [], 'test_r_squared': []
        }
    else:
        history = {
            'train_loss': [], 'train_accuracy': [], 'train_precision': [], 'train_recall': [], 'train_f1': [],
            'test_loss': [], 'test_accuracy': [], 'test_precision': [], 'test_recall': [], 'test_f1': []
        }
    
    model_artifacts_dir = os.path.join(log_dir, 'model_artifacts')
    os.makedirs(model_artifacts_dir, exist_ok=True)
    
    save_hyperparameters(hyperparameters, model_artifacts_dir, model)
    
    early_stopping = EarlyStopping(patience=patience, verbose=True)
    
    for epoch in tqdm(range(epochs)):
        train_metrics = train_step(model=model,
                                 dataloader=train_dataloader,
                                 loss_fn=loss_fn,
                                 optimizer=optimizer,
                                 scheduler=scheduler,
                                 device=device,
                                 is_regression=is_regression)
        
        test_metrics = test_step(model=model,
                               dataloader=test_dataloader,
                               loss_fn=loss_fn,
                               device=device,
                               is_regression=is_regression)

        if is_regression:
             history['train_loss'].append(train_metrics['loss'])
             history['train_mae'].append(train_metrics['mae'])
             history['train_mse'].append(train_metrics['mse'])
             history['train_r_squared'].append(train_metrics['r_squared'])
             
             history['test_loss'].append(test_metrics['loss'])
             history['test_mae'].append(test_metrics['mae'])
             history['test_mse'].append(test_metrics['mse'])
             history['test_r_squared'].append(test_metrics['r_squared'])
        else:
            history['train_loss'].append(train_metrics['loss'])
            history['train_accuracy'].append(train_metrics['accuracy'])
            history['train_precision'].append(train_metrics['precision'])
            history['train_recall'].append(train_metrics['recall'])
            history['train_f1'].append(train_metrics['f1'])
            
            history['test_loss'].append(test_metrics['loss'])
            history['test_accuracy'].append(test_metrics['accuracy'])
            history['test_precision'].append(test_metrics['precision'])
            history['test_recall'].append(test_metrics['recall'])
            history['test_f1'].append(test_metrics['f1'])

        if is_regression:
             writer.add_scalar("train_loss", train_metrics['loss'], epoch)
             writer.add_scalar("train_mae", train_metrics['mae'], epoch)
             writer.add_scalar("train_mse", train_metrics['mse'], epoch)
             writer.add_scalar("train_r_squared", train_metrics['r_squared'], epoch)

             writer.add_scalar("test_loss", test_metrics['loss'], epoch)
             writer.add_scalar("test_mae", test_metrics['mae'], epoch)
             writer.add_scalar("test_mse", test_metrics['mse'], epoch)
             writer.add_scalar("test_r_squared", test_metrics['r_squared'], epoch)
        else:
             writer.add_scalar("train_loss", train_metrics['loss'], epoch)
             writer.add_scalar("train_accuracy", train_metrics['accuracy'], epoch)
             writer.add_scalar("train_precision", train_metrics['precision'], epoch)
             writer.add_scalar("train_recall", train_metrics['recall'], epoch)
             writer.add_scalar("train_f1", train_metrics['f1'], epoch)

             writer.add_scalar("test_loss", test_metrics['loss'], epoch)
             writer.add_scalar("test_accuracy", test_metrics['accuracy'], epoch)
             writer.add_scalar("test_precision", test_metrics['precision'], epoch)
             writer.add_scalar("test_recall", test_metrics['recall'], epoch)
             writer.add_scalar("test_f1", test_metrics['f1'], epoch)
        
        early_stopping(test_metrics['loss'], model, optimizer, scheduler, epoch, model_artifacts_dir)
        
        if early_stopping.early_stop:
            break
        
        if (epoch + 1) % save_interval == 0 or epoch == epochs - 1:
            save_model_state(model, optimizer, scheduler, epoch, model_artifacts_dir)
        
        if epoch % 10 == 0:
            print(f"\nEpoch {epoch}")
            if is_regression:
                 print(f"Train - Loss: {train_metrics['loss']:.4f}, MAE: {train_metrics['mae']:.4f}, R2: {train_metrics['r_squared']:.4f}")
                 print(f"Test  - Loss: {test_metrics['loss']:.4f}, MAE: {test_metrics['mae']:.4f}, R2: {test_metrics['r_squared']:.4f}")
            else:
                 print(f"Train - Loss: {train_metrics['loss']:.4f}, Accuracy: {train_metrics['accuracy']:.4f}, F1: {train_metrics['f1']:.4f}")
                 print(f"Test  - Loss: {test_metrics['loss']:.4f}, Accuracy: {test_metrics['accuracy']:.4f}, F1: {test_metrics['f1']:.4f}")


    
    if is_regression:
         final_metrics_text = (
             f"# Final Model Performance\n\n"
             f"## Training Set\n"
             f"* **Loss**: {history['train_loss'][-1]:.4f}\n"
             f"* **MAE**: {history['train_mae'][-1]:.4f}\n"
             f"* **MSE**: {history['train_mse'][-1]:.4f}\n"
             f"* **R-squared**: {history['train_r_squared'][-1]:.4f}\n\n"
             f"## Test Set\n"
             f"* **Loss**: {history['test_loss'][-1]:.4f}\n"
             f"* **MAE**: {history['test_mae'][-1]:.4f}\n"
             f"* **MSE**: {history['test_mse'][-1]:.4f}\n"
             f"* **R-squared**: {history['test_r_squared'][-1]:.4f}\n"
         )
    else:
         final_metrics_text = (
             f"# Final Model Performance\n\n"
             f"## Training Set\n"
             f"* **Loss**: {history['train_loss'][-1]:.4f}\n"
             f"* **Accuracy**: {history['train_accuracy'][-1]:.4f}\n"
             f"* **Precision**: {history['train_precision'][-1]:.4f}\n"
             f"* **Recall**: {history['train_recall'][-1]:.4f}\n"
             f"* **F1**: {history['train_f1'][-1]:.4f}\n\n"
             f"## Test Set\n"
             f"* **Loss**: {history['test_loss'][-1]:.4f}\n"
             f"* **Accuracy**: {history['test_accuracy'][-1]:.4f}\n"
             f"* **Precision**: {history['test_precision'][-1]:.4f}\n"
             f"* **Recall**: {history['test_recall'][-1]:.4f}\n"
             f"* **F1**: {history['test_f1'][-1]:.4f}\n"
         )
    writer.add_text("final_metrics", final_metrics_text)
    
    return history


class HyperparameterTuner:
    def __init__(self, base_hyperparameters, param_grid, experiment_name, language):

        self.base_hyperparameters = base_hyperparameters
        self.param_grid = param_grid
        self.experiment_name = experiment_name
        self.language = language
        self.label_type = base_hyperparameters.get('LABEL_TYPE', 'vuln_score')
        self.scanner_type = base_hyperparameters.get('SCANNER_TYPE', 'semgrep')
        self.results = []
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.best_model = None
        self.best_model_params = None
        self.best_mae = float('inf') 
        self.best_log_dir = None
        self.is_regression = base_hyperparameters.get('LABEL_TYPE', 'vuln_score') != 'is_vulnerable'
        if not self.is_regression:
             self.best_f1 = 0.0
        
        self._setup_dataloaders()
        
    def _setup_dataloaders(self):
        
        self.train_dataloader, self.test_dataloader = create_dataloaders(
            batch_size=self.base_hyperparameters['BATCH_SIZE'],
            language=self.language,
            label_type=self.label_type,
            scanner_type=self.scanner_type
        )

    def _create_param_combinations(self):

        keys = self.param_grid.keys()
        values = self.param_grid.values()
        
        combinations = list(itertools.product(*values))
        
        param_combinations = []
        for combo in combinations:
            param_dict = self.base_hyperparameters.copy()
            for i, key in enumerate(keys):
                param_dict[key] = combo[i]
            param_combinations.append(param_dict)
        
        return param_combinations
    
    def _train_model(self, hyperparams, trial_num):

        if hyperparams['BATCH_SIZE'] != self.train_dataloader.batch_size:
            self.train_dataloader, self.test_dataloader = create_dataloaders(
                batch_size=hyperparams['BATCH_SIZE'],
                language=self.language,
                label_type=self.label_type,
                scanner_type=self.scanner_type
            )
        
        model = SimpleNN(
            input_size=hyperparams['INPUT_SIZE'],
            hidden_size=hyperparams['HIDDEN_SIZES'],
            dropout_rate=hyperparams['DROPOUT']
        ).to(self.device)
        
        optimizer = torch.optim.Adam(
            params=model.parameters(), 
            lr=hyperparams['LEARNING_RATE']
        )
        
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, 
            step_size=hyperparams['SCHEDULER_STEP_SIZE'], 
            gamma=hyperparams['SCHEDULER_GAMMA']
        )
        
        hidden_sizes_str = "-".join([str(size) for size in hyperparams['HIDDEN_SIZES']])
        description = f"trial-{trial_num}_arch-{hidden_sizes_str}_batch-{hyperparams['BATCH_SIZE']}_lr-{hyperparams['LEARNING_RATE']}_dropout-{hyperparams['DROPOUT']}_scanner-{self.scanner_type}"
        
        hyperparams['LANGUAGE'] = self.language
        hyperparams['SCANNER_TYPE'] = self.scanner_type
        
        writer, log_dir = EnhancedCustomWriter.create_Writer(
            hyperparams, 
            self.experiment_name, 
            model_name,
            extra=description
        )
        

        start_time = timer()
        results = train(
            model=model,
            train_dataloader=self.train_dataloader,
            test_dataloader=self.test_dataloader,
            loss_fn=hyperparams['LOSS_FUNCTION'],
            optimizer=optimizer,
            scheduler=scheduler,
            device=self.device,
            epochs=hyperparams['EPOCHS'],
            writer=writer,
            log_dir=log_dir,
            patience=hyperparams['PATIENCE'],
            is_regression=self.is_regression,
            hyperparameters=hyperparams
        )
        end_time = timer()
        train_time = end_time - start_time
        
        metrics_path = os.path.join(log_dir, 'model_artifacts', 'training_results.json')
        with open(metrics_path, 'w') as f:
            serializable_results = {}
            for key, value in results.items():
                serializable_results[key] = [float(v) for v in value]
            json.dump(serializable_results, f, indent=4)
        
        writer.close()
        
        if not results or not results.get('train_loss'):
            final_metrics = {key: float('nan') for key in [
                'final_train_loss', 'final_train_mae', 'final_train_mse', 'final_train_r_squared', 'final_train_accuracy', 'final_train_precision', 'final_train_recall', 'final_train_f1',
                'final_test_loss', 'final_test_mae', 'final_test_mse', 'final_test_r_squared', 'final_test_accuracy', 'final_test_precision', 'final_test_recall', 'final_test_f1'
            ]}
        else:
            last_epoch_idx = -1
            final_metrics = {'final_train_loss': results['train_loss'][last_epoch_idx], 'final_test_loss': results['test_loss'][last_epoch_idx]}
            if self.is_regression:
                final_metrics.update({
                    'final_train_mae': results['train_mae'][last_epoch_idx],
                    'final_train_mse': results['train_mse'][last_epoch_idx],
                    'final_train_r_squared': results['train_r_squared'][last_epoch_idx],
                    'final_test_mae': results['test_mae'][last_epoch_idx],
                    'final_test_mse': results['test_mse'][last_epoch_idx],
                    'final_test_r_squared': results['test_r_squared'][last_epoch_idx],
                })
            else:
                final_metrics.update({
                    'final_train_accuracy': results['train_accuracy'][last_epoch_idx],
                    'final_train_precision': results['train_precision'][last_epoch_idx],
                    'final_train_recall': results['train_recall'][last_epoch_idx],
                    'final_train_f1': results['train_f1'][last_epoch_idx],
                    'final_test_accuracy': results['test_accuracy'][last_epoch_idx],
                    'final_test_precision': results['test_precision'][last_epoch_idx],
                    'final_test_recall': results['test_recall'][last_epoch_idx],
                    'final_test_f1': results['test_f1'][last_epoch_idx],
                })

        result_entry = {
            'trial': trial_num,
            'hidden_sizes': str(hyperparams['HIDDEN_SIZES']),
            'batch_size': hyperparams['BATCH_SIZE'],
            'learning_rate': hyperparams['LEARNING_RATE'],
            'dropout': hyperparams['DROPOUT'],
            'loss_function': hyperparams['LOSS_FUNCTION'].__class__.__name__,
            'epochs': hyperparams['EPOCHS'],
            **final_metrics,
            'training_time': train_time,
            'log_dir': log_dir
        }
        
        return model, result_entry, log_dir
    
    def run_tuning(self):
        param_combinations = self._create_param_combinations()
        
        for i, params in enumerate(param_combinations):
            trial_num = i + 1
            try:
                model, result, log_dir = self._train_model(params, trial_num)
                self.results.append(result)
                
                if self.is_regression:
                    current_test_mae = result.get('final_test_mae', float('inf'))
                    if current_test_mae < self.best_mae:
                        self.best_mae = current_test_mae 
                        self.best_model = model 
                        self.best_model_params = params
                        self.best_log_dir = log_dir
                else: 
                    current_test_f1 = result.get('final_test_f1', 0.0)
                    if current_test_f1 > self.best_f1: 
                        self.best_f1 = current_test_f1 
                        self.best_model = model 
                        self.best_model_params = params
                        self.best_log_dir = log_dir

                best_model_path = os.path.join("best_models", f"best_model_vulnerability_{self.language}_{self.label_type}_{self.scanner_type}.pt")
                os.makedirs(os.path.dirname(best_model_path), exist_ok=True)
                if self.best_model: torch.save(self.best_model.state_dict(), best_model_path)

                best_params_path = os.path.join("best_models", f"best_params_vulnerability_{self.language}_{self.label_type}_{self.scanner_type}.json")
                if self.best_model_params:
                    with open(best_params_path, 'w') as f:
                        serializable_params = {}
                        for key, value in self.best_model_params.items():
                            if isinstance(value, type) or callable(value): serializable_params[key] = str(value)
                            elif key == 'LOSS_FUNCTION': serializable_params[key] = value.__class__.__name__
                            elif key == 'HIDDEN_SIZES': serializable_params[key] = value
                            else: serializable_params[key] = value
                        if self.best_model: serializable_params['MODEL_ARCHITECTURE'] = str(self.best_model)
                        json.dump(serializable_params, f, indent=4)

            except Exception as e:
                traceback.print_exc()
                self.results.append({
                    'trial': trial_num, 
                    'status': 'failed', 
                    'error': str(e), 
                    'params': {k: (str(v) if isinstance(v, (type, nn.Module)) else v) for k, v in params.items()} 
                })

            self._save_results()
        

        if self.best_model_params:
            if self.is_regression:
                print(f"Best Test MAE: {self.best_mae:.4f}")
            else:
                print(f"Best Test F1 Score: {getattr(self, 'best_f1', 0.0):.4f}") 
            for key, value in self.best_model_params.items():
                 if key not in ['LOSS_FUNCTION', 'OPTIMIZER', 'SCHEDULER']: print(f"  {key}: {value}")
                 elif key == 'LOSS_FUNCTION': print(f"  LOSS_FUNCTION: {value.__class__.__name__}")
        else:
            print("No successful trials completed or no best model found.")

        return {
            'model': self.best_model,
            'hyperparameters': self.best_model_params,
            'best_metric_value': self.best_mae if self.is_regression else getattr(self, 'best_f1', 0.0), 
            'log_dir': self.best_log_dir,
            'all_results': self.results 
        }
    
    def _save_results(self):

        successful_results = [r for r in self.results if r.get('status') != 'failed']
        
        if not successful_results:
             results_df = pd.DataFrame(self.results) 
        else:
            results_df = pd.DataFrame(successful_results) 

        results_path = f"hyperparameter_tuning_results_vulnerability_{self.language}_{self.label_type}_{self.scanner_type}.csv"
        results_df.to_csv(results_path, index=False)
        
        markdown_path = f"hyperparameter_tuning_results_vulnerability_{self.language}_{self.label_type}_{self.scanner_type}.md"
        with open(markdown_path, 'w') as f:
            f.write(f"# Hyperparameter Tuning Results - Vulnerability Prediction - {self.language} - {self.label_type} - {self.scanner_type}\n\n")
            f.write(f"Date: {dt.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            if successful_results: 
                if self.is_regression:
                    cols = ['trial', 'hidden_sizes', 'batch_size', 'learning_rate', 'dropout', 'loss_function',
                            'final_test_mae', 'final_test_mse', 'final_test_r_squared', 
                            'final_train_mae', 'final_train_mse', 'final_train_r_squared', 'training_time']
                    f.write("| " + " | ".join([c.replace('_', ' ').title() for c in cols]) + " |\n")
                    f.write("|-" + "-|".join(['-' * len(c.replace('_', ' ').title()) for c in cols]) + "-|\n")
                    sorted_results = sorted(successful_results, key=lambda x: x.get('final_test_mae', float('inf')))
                    for result in sorted_results:
                        row_values = []
                        for col in cols:
                            val = result.get(col, 'N/A')
                            if isinstance(val, float): row_values.append(f"{val:.4f}")
                            else: row_values.append(str(val))
                        f.write("| " + " | ".join(row_values) + " |\n")
                else:
                    cols = ['trial', 'hidden_sizes', 'batch_size', 'learning_rate', 'dropout', 'loss_function',
                            'final_test_f1', 'final_test_accuracy', 'final_test_precision', 'final_test_recall',
                            'final_train_f1', 'final_train_accuracy', 'final_train_precision', 'final_train_recall', 'training_time']
                    f.write("| " + " | ".join([c.replace('_', ' ').title() for c in cols]) + " |\n")
                    f.write("|-" + "-|".join(['-' * len(c.replace('_', ' ').title()) for c in cols]) + "-|\n")
                    sorted_results = sorted(successful_results, key=lambda x: x.get('final_test_f1', 0.0), reverse=True)
                    for result in sorted_results:
                        row_values = []
                        for col in cols:
                            val = result.get(col, 'N/A')
                            if isinstance(val, float): row_values.append(f"{val:.4f}")
                            else: row_values.append(str(val))
                        f.write("| " + " | ".join(row_values) + " |\n")
            else:
                 f.write("No successful trials to report.\n")

            failed_trials = [r for r in self.results if r.get('status') == 'failed']
            if failed_trials:
                f.write("\n## Failed Trials\n\n")
                f.write("| Trial | Params | Error |\n")
                f.write("|-------|--------|-------|\n")
                for failed in failed_trials:
                    params_str = json.dumps(failed.get('params', {}))
                    f.write(f"| {failed.get('trial', 'N/A')} | `{params_str}` | {failed.get('error', 'Unknown')} |\n")
        

def create_dataloaders(batch_size, language, label_type, scanner_type):

    if language == "Java":
        test_dataset = Repo_Java_Dataset(label_select=label_type, sast_select=scanner_type, split=1.0, type="train")
    elif language == "C":
        test_dataset = Repo_C_Dataset(label_select=label_type, sast_select=scanner_type, split=1.0, type="train")

    elif language == "CPP":
        test_dataset = Repo_CPP_Dataset(label_select=label_type, sast_select=scanner_type, split=1.0, type="train")
    else:
        raise ValueError(f"Unsupported language: {language}. Choose from Java, C, or CPP.")
    test_dataloader = torch.utils.data.DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,
        pin_memory=True
    )
    
    return  test_dataloader

def main():

    

    parser = argparse.ArgumentParser(description='Run hyperparameter tuning for vulnerability prediction')
    parser.add_argument('--tuning', action='store_true', help='Run hyperparameter tuning')
    parser.add_argument('--language', type=str, default='Java', choices=['Java', 'C', 'CPP'], 
                        help='Programming language to use (Java, C, or CPP)')
    parser.add_argument('--label-type', type=str, default='vuln_score',
                        choices=['vuln_score', 'vuln_count', 'vuln_score_detailed', 'is_vulnerable'],
                        help='Type of label to use for training')
    parser.add_argument('--scanner-type', type=str, default='semgrep',
                        choices=['semgrep', 'horusec'],
                        help='Type of vulnerability scanner used (semgrep or horusec)')
    
    args = parser.parse_args()
    
    hyperparameters = {}

    hyperparameters['LANGUAGE'] = args.language
    hyperparameters['LABEL_TYPE'] = args.label_type
    hyperparameters['SCANNER_TYPE'] = args.scanner_type
    
   
        
    os.makedirs("best_models", exist_ok=True)
    
    base_hyperparameters = hyperparameters.copy()

    if args.label_type == "is_vulnerable":
        param_grid = {
            "HIDDEN_SIZES": [[256, 128, 64, 32, 16]],
            "BATCH_SIZE": [8],
            "LEARNING_RATE": [0.001],
            "DROPOUT": [0.2],
            "SCHEDULER_GAMMA": [0.5],
            "OPTIMIZER": [torch.optim.Adam],
            "MOMENTUM": [0.9, 0.99],
            "BETAS": [(0.9, 0.999),],
            "LOSS_FUNCTION": [nn.BCELoss()]
        }
    else:
        param_grid = {
            "HIDDEN_SIZES": [[256, 128, 64, 32, 16]],
            "BATCH_SIZE": [8],
            "LEARNING_RATE": [0.005],
            "DROPOUT": [ 0.2],
            "SCHEDULER_GAMMA": [0.8],
            "OPTIMIZER": [torch.optim.Adam],
            "MOMENTUM": [0.9],
            "BETAS": [(0.9, 0.999)],
            "LOSS_FUNCTION": [nn.L1Loss()]
        }
    tuner = HyperparameterTuner(
        base_hyperparameters=base_hyperparameters,
        param_grid=param_grid,
        experiment_name="Vulnerability_Prediction",
        language=args.language
    )
    
    try:
        results = tuner.run_tuning()

        best_model_path = os.path.join("best_models", f"best_model_vulnerability_{args.language}_{args.label_type}_{args.scanner_type}.pt")
        best_params_path = os.path.join("best_models", f"best_params_vulnerability_{args.language}_{args.label_type}_{args.scanner_type}.json")
        
        if not os.path.exists(best_model_path) and results['model'] is not None:
            torch.save(results['model'].state_dict(), best_model_path)
            
        if not os.path.exists(best_params_path) and results['hyperparameters'] is not None:
            serializable_params = {}
            for key, value in results['hyperparameters'].items():
                if isinstance(value, type) or callable(value):
                    serializable_params[key] = str(value)
                elif key == 'LOSS_FUNCTION':
                    serializable_params[key] = value.__class__.__name__
                elif key == 'HIDDEN_SIZES': serializable_params[key] = value
                else:
                    serializable_params[key] = value
            
            if results['model'] is not None:
                serializable_params['MODEL_ARCHITECTURE'] = str(results['model'])
            
            with open(best_params_path, 'w') as f:
                json.dump(serializable_params, f, indent=4)
        
        return 0
    except Exception as e:
        print(f"Error during hyperparameter tuning: {str(e)}")
        traceback.print_exc()
        print("Hyperparameter tuning failed. Exiting.")
        return 1

if __name__ == "__main__":
    main()